---
title: "Week 12: Model Selection"
author: "Caren Goldberg"
date: "`r Sys.Date()`"
show_toc: true
output:
  knitr:::html_vignette:
    toc: yes
    fig_width: 4 
    fig_height: 3.5
vignette: >
  %\VignetteIndexEntry{Week 12: Model Selection}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
## 1. Overview of Worked Example

### a) Goals and Background

**Goals**: The goal of this worked example is for you to become familiar with creating and analyzing landscape genetic hypotheses using maximum likelihood population effect models (MLPE; Van Strien et al. 2012) and information theory. With this approach, we focus on evaluating evidence for the influence of the (intervening) landscape on link-based metrics (i.e. genetic distance). Incorporating node-based effects (i.e. pond size, etc.) will be covered in the gravity modeling lab. 

We will re-analyze a dataset from Goldberg and Waits 2010 using a maximum likelihood population effect model (MLPE). 

### b) Data set

In previous labs, you have created genetic distance matrices, so here we are going to start with those data already complete, as are the landscape data. For MLPE, two vectors representing the nodes at the ends of each link must be created. These are already included in the data set (`pop1` and `pop2`). 

Each row in the file 'CSF_network.csv' represents a link between two pairs of sampling locations. The columns contain the following variables:

- **Id**: Link ID.
- **Name**: Link name (combines the two site names).
- **logDc.km**: Genetic distance per geographic distance, log transformed (response variable).
- **LENGTH**: Euclidean distance.
- **slope**: Average slope along the link.
- **solarinso**: Average solar insolation along the link.
- **soils**: Dominant soil type along the link (categorical).
- **ag**: Proportion of agriculture along the link.
- **grass**: Proportion of grassland along the link.
- **shrub**: Proportion of shrubland along the link.
- **hi**: Proportion of high density forest along the link. 
- **lo**: Proportion of low density forest along the link.
- **dev**: Proportion of development (buildings) along the link.
- **forest**: Proportion of forest (the sum of hi and lo) along the link.
- **pop1**: 'From' population.
- **pop2**: 'To' population.

### c) Required R libraries

All required packages should have been installed already when you installed 'LandGenCourse'.

```{r packages global_options, include=TRUE, results="hide", message=FALSE, warning=FALSE}
library(LandGenCourse)
library(ggplot2)
#require(lme4)
#require(usdm)
```

### d) Import data

First, read in the dataset. It is named CSF (Columbia spotted frog) to differentiate it from the RALU dataset you have used in previous labs (same species, very different landscapes). 

```{r}
CSFdata <- read.csv(system.file("extdata", "CSF_network.csv", 
                                        package = "LandGenCourse"))
head(CSFdata) 
```

Take a look at the column names and make sure you and R agree on what everything is called, and on the data types (e.g., factor vs. character).

```{r}
str(CSFdata)
```

- `Name` and `soils` are interpreted as factors,
- `Id`, `pop1` and `pop2` are vectors of integers, and
- everything else is a numeric vector, i.e., continuous variable. 

The response Y in our MLPE models will be `logDc.km`, the genetic distance per geographic distance, log transformed to meet normality assumptions. 

**NOTE HW: Please explain why you ar using genetic distance per geographic distance, not just genetic distance. Is this  away of building IBD into the null model? What does it mean for how ecologial distances need to be scaled, i.e., do they need to be expressed per km as well? Would this make sense if it is compositional data, such as percent forest along a link?**

For this exercise, you will test 5 alternative hypotheses. At the end, you will be invited to create your own hypotheses from these variables and test support for them.

**NOTE HW: Are this 48 links retained from a graph? There are 20 sites, I believe, but not all links are represented. How about adding a map?**


## 2. Fitting candidate models

### a) Alternative hypotheses

In information theory, we evaluate evidence for a candidate set of hypotheses that we develop from the literature and theory. Although it is mathematically possible (and often practiced) to fit all possible combinations of input variables, that approach is not consistent with the methodology (see Burnham and Anderson 2002 Chapter 8, the reading for this unit, for more detail). Here, your "a priori"" set of hypotheses (as in the Week 12 Conceptual Exercise) is as follows:

1.	**Full model**: solarinso, forest, ag, shrub, dev
2.	**Landcover model**: ag, shrub, forest, and dev
3.	**Human footprint model**: ag, dev
4.	**Energy conservation model**: slope, shrub, dev
5.	**Historical model**: soils, slope, solarinso

### b) Prepare for model fitting

Because link data are not independent (i.e. the genetic diversity at node 1 influences both the node1/node2 genetic distance and that from node1/node3, etc.), we need to develop a set of random effects to account for this dependency, so that we seperate the pattern due to this covariance from the signal of landscape influence on genetic distance. We start by creating matrices from the `pop1` and `pop2` variables:

Create the `Zl` and `ZZ` matrices:

```{r}
Zl <- lapply(c("pop1","pop2"), function(nm) 
  Matrix:::fac2sparse(CSFdata[[nm]], "d", drop=FALSE))
ZZ <- Reduce("+", Zl[-1], Zl[[1]])
```

Note: The list 'Zl' contains two sparse matrices, one for the 'from' node in 'pop1' and one for the 'to' node in 'pop2'. Each sparse matrix has 20 rows (for the 20 populations) and 48 columns (for the 48 links). Each column has a single value of '1' that indicates the 'from' or 'to' population of the corresponding link. The matrix 'ZZ' is again a sparse matrix that is the sum of the two sparse matrices in 'Zl'. Each column thus has two values of '1', one for the 'from' and one for the 'to' node.

Now we can fit an `lmer` model (linear mixed model; see Week 6 videos) to the data. 

```{r}
mod1 <- lme4::lFormula(logDc.km ~ solarinso + forest + ag + shrub + 
                       dev + (1|pop1), data = CSFdata, REML = TRUE)
```

At this point an error message appears: "Warning message: Some predictor variables are on very different scales: consider rescaling". 

Which variable is causing this problem? To check, get R to show you the first few rows of the dataset:

```{r}
head(CSFdata)
```

One of these variables is a lot larger than the others, but has a small range. This often happens with variables such as easting and can cause issues for model fit. The common solution is to z-transform (standardize) the data, which is conveniently done in R using the `scale` function.

Note: we could use the scale function without specifying the arguments 'center' or 'scale', as both are 'TRUE' by default. The argument 'center=TRUE' centers the variable by subtracting the mean, the 'scale=TRUE' argument rescales the variables to unit variance by dividing each value by the standard deviation. The resulting variable has a mean of zero and a standard deviation and variance of one. 

```{r}
solarinsoz <- scale(CSFdata$solarinso, center=TRUE, scale=TRUE)
```

We then can attach these data back to our dataframe and check to make sure this worked:

```{r}
CSFdata <- cbind(CSFdata, solarinsoz)
names(CSFdata)
```

If `solarinsoz` is there, you are good.

While we are at it, we will make sure that these variables do not have issues with multicollinearity.

Make a dataframe of just the variables to test (note, you cannot use factors here):

```{r}
CSF.df <- with(CSFdata, data.frame(solarinsoz, forest, dev, shrub, ag))
usdm::vif(CSF.df)
```

If you get an error that there is no package called 'usdm', use install.packages("usdm")

Numbers less than 10, or 3, or 4, depending on who you ask, are considered to not be collinear enough to affect model outcomes. So we are good to go here. What would happen if we added grass to this?

```{r}
usdm::vif(cbind(CSF.df, grass=CSFdata$grass))
```

**Question 1**:	Why would adding in the last land cover cause a high amount of collinearity? Consider how these data were calculated.

**NOTE HW: I added the text and code below. As we don't have a video to introduce the concepts for the lab, maybe this additional explanation would be helpful?**

The variables 'forest', 'dev', 'shrub', 'ag' and 'grass' together cover almost all land cover types in the study area. Each is measured as percent area, and together, they make up 100% or close to 100% of the land cover types along each link. Thus if we know e.g. that all 'forest', 'dev', 'shrub' and 'ag' together cover 80% along a link, then it is a good bet that 'grass' will cover the remaining 20%. 

With such sets of compositional data, we need to omit one variable to avoid multi-collinearity (where a variable is a linear combination of other variables), even though the pairwise correlations may be low. For example, there the variable 'grass' showed relatively low correlations with the other variables:

```{r}
cor(CSF.df, CSFdata$grass)
```

### c) Fit all five models

Now we have to remake the modeling objects with our new and improved data frame, but referring to our z-transformed data (thanks to Martin Van Strien and Helene Wagner for the base model code):

```{r}
Zl <- lapply(c("pop1","pop2"), function(nm) 
  Matrix:::fac2sparse(CSFdata[[nm]], "d", drop=FALSE))
ZZ <- Reduce("+", Zl[-1], Zl[[1]])
```

Fit an lmer model to the data:

```{r}
mod_1 <- lme4::lFormula(logDc.km ~ solarinsoz + forest + ag + shrub + 
                        dev + (1|pop1), data = CSFdata, REML = TRUE)
```

In the fitted model replace Zt slot:

**NOTE HW: What is the Zt slot?**

```{r}
mod_1$reTrms$Zt <- ZZ
```

Refit the model (this involves a few steps of optimization starting from the previously fitted model `mod_1`):

```{r}
dfun <- do.call(lme4::mkLmerDevfun, mod_1)
opt <- lme4::optimizeLmer(dfun)
mod1 <- lme4::mkMerMod(environment(dfun), opt, mod_1$reTrms, fr = mod_1$fr)
summary(mod1)
```

**NOTE HW: Can you please explain the output? How would we get p-values, or if we don't do that, why not? Would we need to refit with ML? Or explain that we will use a CI approach later to identify which variables have a 'significant' effect?**

This is the fullest model in our dataset (although our models are not completely nested), so we will take a look at the residuals:

```{r}
plot(mod1) 
```

Residuals are centered around zero and do not show large groupings or patterns, although there is some increase in variation at larger numbers (so the model is having a more difficult time predicting larger genetic distances per km). 

There are many more checks that can be done at this point (referenced in the lecture), but for now we are going to leave evaluating model fit at this and fit the other models.

**NOTE HW: I think it would be useful to have some more plots here to have the code, as 'LandGenCourse' will be available independently from the DGS lectures?**

Now we will make a function (called MPLE) to fit the rest of our models, so that we are not typing the same code over and over (which causes errors and long code that is difficult to navigate or update):


```{r}
MLPE <- function(variables, data) {
  mod2 <- lme4::lFormula(variables, data = data, REML = TRUE)
  dfun <- do.call(lme4::mkLmerDevfun, mod2)
  opt <- lme4::optimizeLmer(dfun)
  mod_2 <- lme4::mkMerMod(environment(dfun), opt, mod2$reTrms,fr = mod2$fr)
  mod2$reTrms$Zt <- ZZ

# Refit the model
  dfun <- do.call(lme4::mkLmerDevfun, mod2)
  opt <- lme4::optimizeLmer(dfun)
  modelout <- lme4::mkMerMod(environment(dfun), opt, mod2$reTrms,fr = mod2$fr)
  return(modelout)
}
```

To use this function, we define variables (the model as we want it to run) and data (the object that contains our data).

```{r}
mod2 <- MLPE(logDc.km ~ forest + ag + shrub + dev + (1|pop1), CSFdata)
mod3 <- MLPE(logDc.km ~ ag + dev + (1|pop1), CSFdata)
mod4 <- MLPE(logDc.km ~ slope + shrub + dev + (1|pop1), CSFdata)
mod5 <- MLPE(logDc.km ~ soils + slope + solarinsoz + (1|pop1), CSFdata)
```

### d) Compare evidence for models

Note: package MuMIn will do this for you, but for this exercise it is useful to see all the pieces of what goes into your evaluation. 

In information theory, we use information criteria (AIC, BIC) to evaluate the relative distance of our hypothesis from truth. For more information, see Burnham and Anderson (2002).

**NOTE HW: See alternative code below, using named list of models.**

**NOTE HW: In Week 6 worked example, we wrote: "The function ‘extractAIC’ refits the models with ‘REML=FALSE’ to obtain AIC values that are comparable between models with different fixed effects (though this does not apply here because the fixed effects were the same), or between models fitted with ‘lm’ and ‘lmer’." Should we use 'extractAIC' here, as we have different fixed effects in the models? 

```{r}
#CSF.IC <- cbind(Model = c(1:5), 
#                AIC = c(AIC(mod1), AIC(mod2), AIC(mod3), 
#                        AIC(mod4), AIC(mod5)), 
#                BIC = c(BIC(mod1), BIC(mod2), BIC(mod3), 
#                        BIC(mod4), BIC(mod5)))
#CSF.IC

# Alternative code HW:
Models <- list(Full=mod1, Landcover=mod2, HumanFootprint=mod3, 
               EnergyConservation=mod4, Historical=mod5)
CSF.IC <- data.frame(extractAIC = sapply(Models, extractAIC)[2,], 
                     AIC = sapply(Models, AIC),
                     BIC = sapply(Models, BIC)) 
CSF.IC
```

We now have some results, great!

Now we will work with these a bit. First, because we do not have an infinite number of samples, we will convert AIC to AICc (which adds a small-sample correction to AIC).

First, find the k parameters used in the model and add them to the table.

**NOTE HW: Again, see alternative code below, using named list of models.**
```{r}
#CSF.IC <- cbind(CSF.IC, 
#                k = c(attr(logLik(mod1), "df"), 
#                      attr(logLik(mod2), "df"), 
#                      attr(logLik(mod3), "df"), 
#                      attr(logLik(mod4), "df"), 
#                      attr(logLik(mod5), "df")))
CSF.IC <- data.frame(CSF.IC, k = sapply(Models, function(ls) attr(logLik(ls), "df")))
CSF.IC
```
**Question 2**:	How does k relate to the number of parameters in each model?

Now, calculate AICc and add it to the dataframe:

```{r}
#CSF.IC<- as.data.frame(CSF.IC)
CSF.IC$AICc <- CSF.IC$AIC + 2*CSF.IC$k*(CSF.IC$k+1)/(48-CSF.IC$k-1)
#CSF.IC <- cbind(CSF.IC, AICc = AICc)
CSF.IC
```

### e) Calculate evidence weights

Next we calculate evidence weights for each model based on AICc and BIC. These can be interpreted as the probability that the model is the closest to truth in the candidate model set. 

Calculate model weights for AICc:

**NOTE HW: Should the label be 'AICew' be 'AICcew'?**

**NOTE HW: See above re 'extractAIC'. Do we need to use it here? I've added it just in case.**

```{r}
extractAICmin <- min(CSF.IC$extractAIC)
RL <- exp(-0.5*(CSF.IC$extractAIC - extractAICmin))
sumRL <- sum(RL)
CSF.IC$extractAICmin <- RL/sumRL

AICcmin <- min(CSF.IC$AICc)
RL <- exp(-0.5*(CSF.IC$AICc - AICcmin))
sumRL <- sum(RL)
CSF.IC$AICew <- RL/sumRL
```

Calculate model weights for BIC:

```{r}
BICmin <- min(CSF.IC$BIC)
RL.B <- exp(-0.5*(CSF.IC$BIC - BICmin))
sumRL.B <- sum(RL.B)
CSF.IC$BICew <- RL.B/sumRL.B
round(CSF.IC,3)
```
**Question 3**:	What did using AICc (rather than AIC) do to inference from these results?

In the multi-model inference approach, we can look across all the models in the dataset and their evidence weights to better understand the system. Here we see that models 1 and 5 have very little evidence of being close to the truth, that model 4 has just a little more, and that models 2 and 3 can be seen as competing for which one is closest to truth, at least if you're looking at AIC. BIC imposes a larger penalty for additional parameters, so it has a larger distinction between these. Let's review what these models are:

1.	**Full model**: solarinso, forest, ag, shrub, dev
2.	**Landcover model**: ag, shrub, forest, and dev
3.	**Human footprint model**: ag, dev
4.	**Energy conservation model**: slope, shrub, dev
5.	**Historical model**: soils, slope, solarinso

### f) Confidence intervals for predictors based on single best model

According to Row et al. (2017), we can use the confidence interval from the variables in the models to identify those that are contributing to landscape resistance. Looking at models 2 and 3, we have some evidence that shrub and forest cover matter, but it's not as strong as the evidence for ag and development. Let's look at the parameter estimates to understand more.

```{r}
#summary(mod2)
summary(Models$Landcover)
```

Now we'll estimate the 95% confidence interval for these estimates:

```{r}
#confint(mod2, level = 0.95, method = "Wald")
confint(Models$Landcover, level = 0.95, method = "Wald")
```

**NOTE HW: I suggest adding the following sencence: "In esssence, we here use the confidence intervals to approximate a statistical significance test."**

The confidence intervals that overlap 0 do not have as much evidence of being important to gene flow, based on simulations in Row et al. (2017). So in this case, there is only strong evidence that agriculture influences the rate of gene flow. Note that because the metric is genetic distance, negative values indicate more gene flow.

**Question 4**:	What evidence is there that shrub and forest matter to gene flow from these analyses? Would you include them in a conclusion about landscape resistance in this system?

**Question 5**:  What evidence is there that development influences gene flow from these analyses? Would you include it in a conclusion about landscape resistance in this system?

**NOTE HW: We dropped 'grass' to avoid multicollinearity. How can we be sure that 'grass' is not important? Should we add a question about this, or a note?**


## 3. Optional: Model averaging

**NOTE HW: What is your take on (a) factor importance, and (b) estimating parameters by weighting across models? I've drafted a brief section that uses functions from MuMIn to calculate model weights, factor importance and model-averaged estimates and CIs. It's up to you if you want to include this or not, and if so, please feel free to edit.**

### a) Model selection with package 'MuMIn'

The package MuMIn makes it easy to create a table for model comparison with the function 'model.sel'. With the argument `rank`, you can specify which criterion to use.

```{r}
Models.table <- MuMIn::model.sel(Models, rank="AICc")
Models.table
```

Note: In the first few columns, the table lists, for each predictor in a model, its parameter estimate. This makes it easy to check to what degree the slope estimates depend on which other variables are in the model. 

### b) Select a subset of models for averaging

We could take this one step further and estimate parameters and their confidence intervals not just from the best model but from a subset of models that have high model weights. Two commonly used definitions of such a subset include:

- `subset=cumsum(weight) <= .95`: The subset of highest-ranked models that together account for at least 95% of model weights. 
- `subset=delta < 4`: The subset of all models that have a delta value greater than 4. 

```{r}
Subset <- MuMIn::get.models(Models.table, cumsum(weight) <= .95)
```

Now we can estimate parameters as a weighted mean of the estimates from the models in the subset, with weights proportional to their model weight.

```{r}
Models.avg <- MuMIn::model.avg(Subset)
Models.avg
```
We get two rows of coefficients. The naming is confusing, as both are calculated from the same subset of models as defined above. Here, subset refers to a subset of parameters, not models:

- **full**: In each model, slope coefficients for predictors that are not included in the model are estimated to be zero and then included in the averaging across models.
- **subset**: In each model, slope coefficients for predictors that are not included in the model are set to NA, i.e., these estimates are not included in the averaging. Thus, for each predictor, coefficient estimates are averaged only across those models that contain the predictor.  

The model selection table now includes only the models in the subset, and the weights are adjusted accordingly (they will now sum to 1 for the subset):

```{r}
Models.avg$msTable
```

### c) Model-averaged confidence intervals for predictors

In addition to averaging coeffient estimates, we can also get their confidence intervals as a weighted average across models. 

Note: as with the coefficient estimates above, `full=FALSE` treats estimates for predictors not included in a model as NAs, whereas `full=FALSE` treats them as zeros.

```{r}
confint(Models.avg, full=FALSE, level = 0.95, method="Wald")
confint(Models.avg, full=TRUE, level = 0.95, method="Wald")
```

**Question 7**:	How did model averaging affect the results and interpretation?

### d) Quantify factor importance given a (sub)set of models

Given the subset of models, we can get a measure of the importance of each predictor by summing the weights of all models that contain the predictor (see Week 7 video). 

```{r}
Models.avg$importance
```

Importance values will depend on the criterion used (e.g., AICc vs. BIC) and on the set or subset of models used. Here we plot importance values from the full set of models and from the subset (both were based on AICc). 

Note: the function `importance` returns an object of class 'importance'. A simple way to extact the values as a named vector is to use the function `c()`. This can then be plotted with the generic function `barplot`. 

```{r fig.show="hold", fig.width=8, fig.height=4}
par(mfrow=c(1,2))
barplot(c(Models.avg$importance), main="Subset of models", 
        las=2, ylim=c(0,1), ylab="Importance")
Importance <- MuMIn::model.avg(Models.table)$importance
barplot(c(Importance), main="All models", las=2, ylim=c(0,1))
```

Or with `ggplot2`:

```{r}
Res <- c(Importance)
ggplot() + aes(x=reorder(names(Res), -Res), y=Res) +
  geom_bar(stat="identity") + xlab("") + ylab("Importance")
```

**Question 8**:	How does factor importance relate to model weights? What would you consider an 'important' predictor?


## 4. Your turn to test additional hypotheses!

Create your own (small) set of hypotheses to rank for evidence using the dataset provided. Modify the code above to complete the following steps: 

1.  Define your hypotheses.
2.  Calculate the VIF table for full model.
3.  Make a residual plot for full model.
4.  Create table of AICc and BIC weights.
5.  Create confidence intervals from models with high evidence weights.

What can you infer from your analysis about what influences gene flow of this species?

```{r message=FALSE, warning=TRUE, include=FALSE}
LandGenCourse::detachAllPackages()
```
