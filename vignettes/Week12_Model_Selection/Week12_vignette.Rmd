---
title: "Week 12: Model Selection"
author: "Caren Goldberg"
date: "`r Sys.Date()`"
show_toc: true
output:
  knitr:::html_vignette:
    toc: yes
    fig_width: 4 
    fig_height: 3.5
vignette: >
  %\VignetteIndexEntry{Week 12: Model Selection}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
## 1. Overview of Worked Example

### a) Goals and Background

**Goals**: The goal of this worked example is for you to become familiar with creating and analyzing landscape genetic hypotheses using maximum likelihood population effect models (MLPE; Van Strien et al. 2012) and information theory. With this approach, we focus on evaluating evidence for the influence of the (intervening) landscape on link-based metrics (i.e. genetic distance). Incorporating node-based effects (i.e. pond size, etc.) will be covered in the gravity modeling lab. 

We will re-analyze a dataset from Goldberg and Waits 2010 using a maximum likelihood population effect model (MLPE). 

### b) Data set

In previous labs, you have created genetic distance matrices, so here we are going to start with those data already complete, as are the landscape data. For MLPE, two vectors representing the nodes at the ends of each link must be created. These are already included in the data set (`pop1` and `pop2`). 

Each row in the file 'CSF_network.csv' represents a link between two pairs of sampling locations. The columns contain the following variables:

- **Id**: Link ID.
- **Name**: Link name (combines the two site names).
- **logDc.km**: Genetic distance per geographic distance, log transformed (response variable).
- **LENGTH**: Euclidean distance.
- **slope**: Average slope along the link.
- **solarinso**: Average solar insolation along the link.
- **soils**: Dominant soil type along the link (categorical).
- **ag**: Proportion of agriculture along the link.
- **grass**: Proportion of grassland along the link.
- **shrub**: Proportion of shrubland along the link.
- **hi**: Proportion of high density forest along the link. 
- **lo**: Proportion of low density forest along the link.
- **dev**: Proportion of development (buildings) along the link.
- **forest**: Proportion of forest (the sum of hi and lo) along the link.
- **pop1**: 'From' population.
- **pop2**: 'To' population.

### c) Required R libraries

All required packages should have been installed already when you installed 'LandGenCourse'.

```{r packages global_options, include=TRUE, results="hide", message=FALSE, warning=FALSE}
library(LandGenCourse)
#require(lme4)
#require(usdm)
```

### d) Import data

First, read in the dataset. It is named CSF (Columbia spotted frog) to differentiate it from the RALU dataset you have used in previous labs (same species, very different landscapes). 

```{r}
CSFdata <- read.csv(system.file("extdata", "CSF_network.csv", 
                                        package = "LandGenCourse"))
View(CSFdata) 
```

Take a look at the column names and make sure you and R agree on what everything is called, and on the data types (e.g., factor vs. character).

```{r}
str(CSFdata)
```

- `Name` and `soils` are interpreted as factors,
- `Id`, `pop1` and `pop2` are vectors of integers, and
- everything else is a numeric vector, i.e., continuous variable. 

The response Y in our MLPE models will be `logDc.km`, the genetic distance per geographic distance, log transformed to meet normality assumptions. 

**NOTE HW: Please explain why you ar using genetic distance per geographic distance, not just genetic distance. Is this  away of building IBD into the null model? What does it mean for how ecologial distances need to be scaled, i.e., do they need to be expressed per km as well? Would this make sense if it is compositional data, such as percent forest along a link?**

For this exercise, you will test 5 alternative hypotheses. At the end, you will be invited to create your own hypotheses from these variables and test support for them.

**NOTE HW: Are this 48 links retained from a graph? There are 20 sites, I believe, but not all links are represented. How about adding a map?**


## 2. Fitting candidate models

### a) Alternative hypotheses

In information theory, we evaluate evidence for a candidate set of hypotheses that we develop from the literature and theory. Although it is mathematically possible (and often practiced) to fit all possible combinations of input variables, that approach is not consistent with the methodology (see Burnham and Anderson 2002 Chapter 8, the reading for this unit, for more detail). Here, your "a priori"" set of hypotheses (as in the Week 12 Conceptual Exercise) is as follows:

1.	**Full model**: solarinso, forest, ag, shrub, dev
2.	**Landcover model**: ag, shrub, forest, and dev
3.	**Human footprint model**: ag, dev
4.	**Energy conservation model**: slope, shrub, dev
5.	**Historical model**: soils, slope, solarinso

### b) Prepare for model fitting

Because link data are not independent (i.e. the genetic diversity at node 1 influences both the node1/node2 genetic distance and that from node1/node3, etc.), we need to develop a set of random effects to account for this dependency, so that we seperate the pattern due to this covariance from the signal of landscape influence on genetic distance. We start by creating matrices from the `pop1` and `pop2` variables:

Create the `Zl` and `ZZ` matrices:

```{r}
Zl <- lapply(c("pop1","pop2"), function(nm) 
  Matrix:::fac2sparse(CSFdata[[nm]], "d", drop=FALSE))
ZZ <- Reduce("+", Zl[-1], Zl[[1]])
```

Note: The list 'Zl' contains two sparse matrices, one for the 'from' node in 'pop1' and one for the 'to' node in 'pop2'. Each sparse matrix has 20 rows (for the 20 populations) and 48 columns (for the 48 links). Each column has a single value of '1' that indicates the 'from' or 'to' population of the corresponding link. The matrix 'ZZ' is again a sparse matrix that is the sum of the two sparse matrices in 'Zl'. Each column thus has two values of '1', one for the 'from' and one for the 'to' node.

Now we can fit an `lmer` model (linear mixed model; see Week 6 videos) to the data. 

```{r}
mod1 <- lme4::lFormula(logDc.km ~ solarinso + forest + ag + shrub + 
                       dev + (1|pop1), data = CSFdata, REML = TRUE)
```

At this point an error message appears: "Warning message: Some predictor variables are on very different scales: consider rescaling". 

Which variable is causing this problem? To check, get R to show you the first few rows of the dataset:

```{r}
head(CSFdata)
```

One of these variables is a lot larger than the others, but has a small range. This often happens with variables such as easting and can cause issues for model fit. The common solution is to z-transform (standardize) the data, which is conveniently done in R using the `scale` function.

Note: we could use the scale function without specifying the arguments 'center' or 'scale', as both are 'TRUE' by default. The argument 'center=TRUE' centers the variable by subtracting the mean, the 'scale=TRUE' argument rescales the variables to unit variance by dividing each value by the standard deviation. The resulting variable has a mean of zero and a standard deviation and variance of one. 

```{r}
solarinsoz <- scale(CSFdata$solarinso, center=TRUE, scale=TRUE)
```

We then can attach these data back to our dataframe and check to make sure this worked:

```{r}
CSFdata <- cbind(CSFdata, solarinsoz)
names(CSFdata)
```

If `solarinsoz` is there, you are good.

While we are at it, we will make sure that these variables do not have issues with multicollinearity.

Make a dataframe of just the variables to test (note, you cannot use factors here):

```{r}
CSF.df <- with(CSFdata, data.frame(solarinsoz, forest, dev, shrub, ag))
usdm::vif(CSF.df)
```

If you get an error that there is no package called 'usdm', use install.packages("usdm")

Numbers less than 10, or 3, or 4, depending on who you ask, are considered to not be collinear enough to affect model outcomes. So we are good to go here. What would happen if we added grass to this?

```{r}
usdm::vif(cbind(CSF.df, grass=CSFdata$grass))
```

**NOTE HW: I added the text and code below, though I see now that you meant this to be a question at the end. As we don't have a video to introduce the concepts for the lab, maybe a bit more explanation would be helpful?**

Why is this? The variables 'forest', 'dev', 'shrub', 'ag' and 'grass' together cover almost all land cover types in the study area. Each is measured as percent area, and together, they make up 100% or close to 100% of the land cover types along each link. Thus if we know e.g. that all 'forest', 'dev', 'shrub' and 'ag' together cover 80% along a link, then it is a good bet that 'grass' will cover the remaining 20%. 

Therefore, with composition data, we need to omit one variable to avoid multi-collinearity (where a variable is a linear combination of other variables), even though the pairwise correlations may be low. For example, there the variable 'grass' showed relatively low correlations with the other variables:

```{r}
cor(CSF.df, CSFdata$grass)
```

### c) Fit all five models

Now we have to remake the modeling objects with our new and improved data frame, but referring to our z-transformed data (thanks to Martin Van Strien and Helene Wagner for the base model code):

```{r}
Zl <- lapply(c("pop1","pop2"), function(nm) 
  Matrix:::fac2sparse(CSFdata[[nm]], "d", drop=FALSE))
ZZ <- Reduce("+", Zl[-1], Zl[[1]])
```

Fit an lmer model to the data:

```{r}
mod_1 <- lme4::lFormula(logDc.km ~ solarinsoz + forest + ag + shrub + 
                        dev + (1|pop1), data = CSFdata, REML = TRUE)
```

In the fitted model replace Zt slot:

**NOTE HW: What is the Zt slot?**

```{r}
mod_1$reTrms$Zt <- ZZ
```

Refit the model (this involves a few steps of optimization starting from the previously fitted model `mod_1`):

```{r}
dfun <- do.call(lme4::mkLmerDevfun, mod_1)
opt <- lme4::optimizeLmer(dfun)
mod1 <- lme4::mkMerMod(environment(dfun), opt, mod_1$reTrms, fr = mod_1$fr)
summary(mod1)
```

**NOTE HW: Can you please explain the output? How would we get p-values, or if we don't do that, why not? Would we need to refit with ML? Or explain that we will use a CI approach later to identify which variables have a 'significant' effect?**

This is the fullest model in our dataset (although our models are not completely nested), so we will take a look at the residuals:

```{r}
plot(mod1) 
```

Residuals are centered around zero and do not show large groupings or patterns, although there is some increase in variation at larger numbers (so the model is having a more difficult time predicting larger genetic distances per km). 

There are many more checks that can be done at this point (referenced in the lecture), but for now we are going to leave evaluating model fit at this and fit the other models.

**NOTE HW: I think it would be useful to have some more plots here to have the code, as 'LandGenCourse' will be available independently from the DGS lectures?**

Now we will make a function (called MPLE) to fit the rest of our models, so that we are not typing the same code over and over (which causes errors and long code that is difficult to navigate or update):


```{r}
MLPE <- function(variables, data) {
  mod2 <- lme4::lFormula(variables, data = data, REML = TRUE)
  dfun <- do.call(lme4::mkLmerDevfun, mod2)
  opt <- lme4::optimizeLmer(dfun)
  mod_2 <- lme4::mkMerMod(environment(dfun), opt, mod2$reTrms,fr = mod2$fr)
  mod2$reTrms$Zt <- ZZ

# Refit the model
  dfun <- do.call(lme4::mkLmerDevfun, mod2)
  opt <- lme4::optimizeLmer(dfun)
  modelout <- lme4::mkMerMod(environment(dfun), opt, mod2$reTrms,fr = mod2$fr)
  return(modelout)
}
```

To use this function, we define variables (the model as we want it to run) and data (the object that contains our data).

```{r}
mod2 <- MLPE(logDc.km ~ forest + ag + shrub + dev + (1|pop1), CSFdata)
mod3 <- MLPE(logDc.km ~ ag + dev + (1|pop1), CSFdata)
mod4 <- MLPE(logDc.km ~ slope + shrub + dev + (1|pop1), CSFdata)
mod5 <- MLPE(logDc.km ~ soils + slope + solarinsoz + (1|pop1), CSFdata)
```

### d) Compare evidence for models

Note: package MuMIn will do this for you, but for this exercise it is useful to see all the pieces of what goes into your evaluation. 

In information theory, we use information criteria (AIC, BIC) to evaluate the relative distance of our hypothesis from truth. For more information, see Burnham and Anderson (2002).

**NOTE HW: See alternative code below, using named list of models.**

```{r}
#CSF.IC <- cbind(Model = c(1:5), 
#                AIC = c(AIC(mod1), AIC(mod2), AIC(mod3), 
#                        AIC(mod4), AIC(mod5)), 
#                BIC = c(BIC(mod1), BIC(mod2), BIC(mod3), 
#                        BIC(mod4), BIC(mod5)))
#CSF.IC

# Alternative code HW:
Models <- list(Full=mod1, Landcover=mod2, HumanFootprint=mod3, 
               EnergyConservation=mod4, Historical=mod5)
CSF.IC <- data.frame(AIC = sapply(Models, AIC),BIC = sapply(Models, BIC)) 
CSF.IC
```

We now have some results, great!

Now we will work with these a bit. First, because we do not have an infinite number of samples, we will convert AIC to AICc (which adds a small-sample correction to AIC).

First, find the k parameters used in the model and add them to the table.

**NOTE HW: Again, see alternative code below, using named list of models.**
```{r}
#CSF.IC <- cbind(CSF.IC, 
#                k = c(attr(logLik(mod1), "df"), 
#                      attr(logLik(mod2), "df"), 
#                      attr(logLik(mod3), "df"), 
#                      attr(logLik(mod4), "df"), 
#                      attr(logLik(mod5), "df")))
CSF.IC <- data.frame(CSF.IC, k = sapply(Models, function(ls) attr(logLik(ls), "df")))
CSF.IC
```

Now, calculate AICc and add it to the dataframe:

```{r}
#CSF.IC<- as.data.frame(CSF.IC)
CSF.IC$AICc <- CSF.IC$AIC + 2*CSF.IC$k*(CSF.IC$k+1)/(48-CSF.IC$k-1)
#CSF.IC <- cbind(CSF.IC, AICc = AICc)
CSF.IC
```

### e) Calculate evidence weights

Next we calculate evidence weights for each model based on AICc and BIC. These can be interpreted as the probability that the model is the closest to truth in the candidate model set. 

Calculate model weights for AICc:

**NOTE HW: Should 'AICew' be 'AICcew'?**

```{r}
AICcmin <- min(CSF.IC$AICc)
RL <- exp(-0.5*(CSF.IC$AICc - AICcmin))
sumRL <- sum(RL)
CSF.IC$AICew <- RL/sumRL
```

Calculate model weights for BIC:

```{r}
BICmin <- min(CSF.IC$BIC)
RL.B <- exp(-0.5*(CSF.IC$BIC - BICmin))
sumRL.B <- sum(RL.B)
CSF.IC$BICew <- RL.B/sumRL.B
round(CSF.IC,3)
```

In the multi-model inference approach, we can look across all the models in the dataset and their evidence weights to better understand the system. Here we see that models 1 and 5 have very little evidence of being close to the truth, that model 4 has just a little more, and that models 2 and 3 can be seen as competing for which one is closest to truth, at least if you're looking at AIC. BIC imposes a larger penalty for additional parameters, so it has a larger distinction between these. Let's review what these models are:

1.	**Full model**: solarinso, forest, ag, shrub, dev
2.	**Landcover model**: ag, shrub, forest, and dev
3.	**Human footprint model**: ag, dev
4.	**Energy conservation model**: slope, shrub, dev
5.	**Historical model**: soils, slope, solarinso

According to Row et al. (2017), we can use the confidence interval from the variables in the models to identify those that are contributing to landscape resistance. Looking at models 2 and 3, we have some evidence that shrub and forest cover matter, but it's not as strong as the evidence for ag and development. Let's look at the parameter estimates to understand more.

```{r}
#summary(mod2)
summary(Models$Landcover)
```

Now we'll estimate the 95% confidence interval for these estimates:

```{r}
#confint(mod2, level = 0.95, method = "Wald")
confint(Models$Landcover, level = 0.95, method = "Wald")
```

The estimates that overlap 0 do not have as much evidence of being important to gene flow, based on simulations in Row et al. (2017). So in this case, there is only strong evidence that agriculture influences the rate of gene flow. Note that because the metric is genetic distance, negative values indicate more gene flow.

**NOTE HW: We dropped 'grass' to avoid multicollinearity. How can we be sure that 'grass' is not important?**

**NOTE HW: Maybe intersperse the questions in the text where they fit? This could make the lab more engaging for users.**

**NOTE HW: What is your take on (a) factor importance, and (b) estimating parameters by weighting across models? Maybe we could add a brief section 'multimodel inference' that uses functions from MuMIn to calculate model weights, factor importance and model-averaged estimates and CIs? It's up to you if you want this or not. If you think it's a good idea but we don't have the time now, we can add it later.**



```{r}
#Models.avg <- MuMIn::model.avg(Models, rank="AICc",
#                               subset=cumsum(weight) <= .95)
Models.avg <- MuMIn::model.avg(Models, rank="AICc",
                               subset=delta < 4)
Models.avg
Models.avg$msTable
Models.avg$coefficients
Models.avg$coefArray
Models.avg$importance
confint(Models.avg)
```

**Questions**:

1.	Why would adding in the last land cover cause a high amount of collinearity? Consider how these data were calculated.
2.	What did using AICc (rather than AIC) do to inference from these results?
3.	How does k relate to the number of parameters in each model?
4.	What evidence is there that shrub and forest matter to gene flow from these analyses? Would you include them in a conclusion about landscape resistance in this system?
5.  What evidence is there that development influences gene flow from these analyses? Would you include it in a conclusion about landscape resistance in this system?


## 3. Your turn to test additional hypotheses!

Create your own (small) set of hypotheses to rank for evidence using the dataset provided. Modify the code above to complete the following steps: 

1.  Define your hypotheses.
2.  Calculate the VIF table for full model.
3.  Make a residual plot for full model.
4.  Create table of AICc and BIC weights.
5.  Create confidence intervals from models with high evidence weights.

What can you infer from your analysis about what influences gene flow of this species?

```{r message=FALSE, warning=TRUE, include=FALSE}
LandGenCourse::detachAllPackages()
```
